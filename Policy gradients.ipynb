{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_policy_gradients_exercise.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mLXw6zd-k3Xd"},"source":["# Homework 3 (15 Pts)"]},{"cell_type":"markdown","metadata":{"id":"1AcEJQ5NZVYW"},"source":["All homeworks are self-contained. They can be completed in their respective notebooks.\n","To edit and re-run code, you can therefore simply edit and restart the code cells below.\n","There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n","This file should automatically be synced with your Google Drive. We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n"," However, you will need to re-mount your Google Drive and re-install packages with every new instance."]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"4HBPnmbIPPyl","executionInfo":{"status":"ok","timestamp":1636110036633,"user_tz":-60,"elapsed":24529,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}},"outputId":"0c335462-162e-4dd2-9843-e309544cbc4d"},"source":["# Your work will be stored in a folder called `drl_ws21` by default to prevent Colab\n","# instance timeouts from deleting your edits.\n","# We do this by mounting your google drive on the virtual machine created in this colab\n","# session. For this, you will likely need to sign in to your Google account and copy a\n","# passcode into a field below\n","\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"OuCfTLJIx5nQ","executionInfo":{"status":"ok","timestamp":1636110036635,"user_tz":-60,"elapsed":22,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}},"outputId":"f9ddfa3b-35fc-42a4-e2f1-b8f8d8248b07"},"source":["# Create paths in your google drive\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/drl_ws21'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","    % mkdir $DRIVE_PATH\n","\n","# the space in `My Drive` causes some issues,\n","# make a symlink to avoid this\n","SYM_PATH = '/content/drl_ws21'\n","if not os.path.exists(SYM_PATH):\n","    !ln -s $DRIVE_PATH $SYM_PATH\n","% cd $SYM_PATH"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/drl_ws21\n"]}]},{"cell_type":"code","metadata":{"id":"SDMWmItLrL1R","pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636110040594,"user_tz":-60,"elapsed":3970,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}},"outputId":"7360de01-245f-4d30-87ed-d520fc1a88ad"},"source":["# Install **python** packages\n","\n","!pip install matplotlib numpy tqdm torch"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"UunygyDXrx7k"},"source":["In this homework we will be mainly working on Policy gradients (Lecture 5) \n","and Natural Policy Gradients (Lecture 6). We are going to implement the \n","REINFORCE, the Policy Gradient Theorem and the Natural Gradient algorithms. \n","\n","We start by importing all the necessary python modules and defining some helper\n","functions which you do not need to change. Still, make sure you are aware of\n","what they do."]},{"cell_type":"code","metadata":{"cellView":"form","id":"enh5ZMHftEO7","pycharm":{"name":"#%%\n"},"executionInfo":{"status":"ok","timestamp":1636110040977,"user_tz":-60,"elapsed":407,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["# Imports and utility\n","# Progress bar\n","from copy import deepcopy\n","import time, os, tqdm\n","\n","# Plotting\n","import matplotlib.pyplot as plt\n","\n","# numerical python\n","import numpy as np\n","\n","# Set random seeds\n","np.random.seed(0)\n","\n","fig_dict = None\n","\n","\n","class ProgressBar:\n","    def __init__(self, num_iterations: int, verbose: bool = True):\n","        if verbose:  # create a nice little progress bar\n","            self.scalar_tracker = tqdm.tqdm(total=num_iterations, desc=\"Scalars\", \n","                                            bar_format=\"{desc}\",\n","                                            position=0, leave=True)\n","            progress_bar_format = '{desc} {n_fmt:' + str(\n","                len(str(num_iterations))) + '}/{total_fmt}|{bar}|{elapsed}<{remaining}'\n","            self.progress_bar = tqdm.tqdm(total=num_iterations, desc='Iteration', \n","                                          bar_format=progress_bar_format,\n","                                          position=1, leave=True)\n","        else:\n","            self.scalar_tracker = None\n","            self.progress_bar = None\n","\n","    def __call__(self, **kwargs):\n","        if self.progress_bar is not None:\n","            formatted_scalars = {key: [f\"{v:.3e}\" for v in value] if isinstance(value, list) \n","                                 else f\"{value:.3e}\"\n","                                 for key, value in kwargs.items()}\n","            description = (\"Scalars: \" + \"\".join([str(key) + \"=\" + value + \", \"\n","                                                  for key, value in formatted_scalars.items()]))[:-2]\n","            self.scalar_tracker.set_description(description)\n","            self.progress_bar.update(1)\n","\n","\n","# specify the path to save the recordings of this run to.\n","data_path = '/content/drl_ws21/exercise_3'\n","data_path = os.path.join(data_path, time.strftime(\"%d-%m-%Y_%H-%M\"))\n","if not (os.path.exists(data_path)):\n","    os.makedirs(data_path)\n","\n","\n","# this function will automatically save your figure into your google drive folder (if correctly mounted!)\n","def save_figure(save_name: str) -> None:\n","    assert save_name is not None, \"Need to provide a filename to save to\"\n","    plt.savefig(os.path.join(data_path, save_name + \".png\"))\n","\n","def save_plots_from_fig_dict(fig_dict, name):\n","    plt.figure(fig_dict[\"Rewards\"].number)\n","    save_figure(name +'_rewards')\n","    \n","    plt.figure(fig_dict[\"Estimated Gradients\"].number)\n","    save_figure(name +'_estimated_gradients')\n","    \n","    plt.figure(fig_dict[\"Contour Plot\"].number)\n","    save_figure(name +'_contour_plot')\n","    \n","\n","def do_contour_plot(env):\n","    contour_plot_policy = LinPolicy()\n","    b_max = 0\n","    b_min = -10\n","    n_points = 50\n","    x = np.linspace(start=b_min, stop=b_max, num=n_points)\n","    y = np.linspace(start=b_min, stop=b_max, num=n_points)\n","    X, Y = np.meshgrid(x, y)\n","    policy_params = np.vstack([X.reshape(-1), Y.reshape(-1)]).T\n","    policy_params = np.concatenate((policy_params, np.ones((policy_params.shape[0], 1))), \n","                                   axis=1)\n","    rews = np.zeros(policy_params.shape[0])\n","    for i in range(policy_params.shape[0]):\n","        c_params = policy_params[i, :]\n","        contour_plot_policy.update_params(c_params)\n","        rews[i] = PG.test_policy(policy=contour_plot_policy, env=env, n_trials=1)\n","    fig, ax = plt.subplots()\n","    Z = np.clip(rews, -1500, 1500).reshape(X.shape)\n","    CS = ax.contour(X, Y, Z)\n","    ax.clabel(CS, inline=True, fontsize=10)\n","    plt.xlabel('k1')\n","    plt.ylabel('k2')\n","    return fig\n","\n","\n","def plot_params_in_contour_plot(fig, parameters, color):\n","    plt.figure(fig.number)\n","    plt.plot(parameters[:, 0], parameters[:, 1], color=color, alpha=0.5)\n","    plt.plot(parameters[:, 0], parameters[:, 1], 'x', color=color, alpha=0.5)\n","    plt.plot(parameters[-1, 0], parameters[-1, 1], 'x', color='r')\n","    return fig\n","\n","\n","def create_fig_dict():\n","    figures_dict = {}\n","    figures_dict[\"Rewards\"] = plt.figure('Rewards')\n","    plt.xlabel('Iterations')\n","    plt.ylabel('Mean Rewards')\n","\n","    figures_dict[\"Estimated Gradients\"] = plt.figure('Estimated Gradients')\n","    plt.xlabel('Iterations')\n","    plt.ylabel('Gradients of Parameters')\n","\n","    figures_dict[\"Contour Plot\"] = None\n","    return figures_dict\n","\n","\n","def finish_training(estimated_grads, parameters, test_rewards, color, fig_dict, \n","                    legend_label=None):\n","    plt.figure(fig_dict[\"Rewards\"].number)\n","    plt.plot(test_rewards, color, label=legend_label)\n","    plt.legend()\n","\n","    plt.figure(fig_dict[\"Estimated Gradients\"].number)\n","    plt.plot(estimated_grads, color, label=legend_label)\n","\n","    plot_params_in_contour_plot(fig_dict[\"Contour Plot\"], parameters, color)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bj_5XyqHZVYZ","pycharm":{"name":"#%% md\n"}},"source":["## **Continuous Control with Linear Controller**"]},{"cell_type":"markdown","metadata":{"id":"LwV468lKGDfT"},"source":["We are going to consider a linear dynamical system with quadratic reward \n","function. For the continous systems part, we will optimize for the parameters of a linear controller. \n","The learning agent does not have any information about the system dynamics and the reward function. \n","\n","### **Linear System and Quadratic Reward Function** \n","\n","The linear dynamics are described as follows:\n","\n","\\begin{align}\n","      \\boldsymbol{s'} = \n","       \\boldsymbol{As} + \\boldsymbol{Ba},\n","\\end{align}\n","where $\\boldsymbol{s'}$ denotes the state in the next time step and $\\boldsymbol{a}$ is the action input to the system. The identites of the system are given as \n","\\begin{align}\n","    \\boldsymbol{A} = \\begin{bmatrix}\n","                        1 & 0.1\\\\\n","                        0 & 0.99\n","                      \\end{bmatrix}, ~~~~~\n","     \\boldsymbol{B} = \\begin{bmatrix}\n","                        0 \\\\\n","                        0.1 \n","                       \\end{bmatrix}.\n","\\end{align}\n","Thus, we have a two dimensional state-space and a one dimensional action space.\n","\n","The immediate reward function, is given as\n","\\begin{align}\n","    r(\\boldsymbol{s_t}, a_t) = -\\boldsymbol{s_t}^T\\boldsymbol{M}\\boldsymbol{s_t} - a_t^2Q,\n","\\end{align}\n","resulting in an episode reward\n","\\begin{align}\n","    R(\\tau) = \\sum_t r(\\boldsymbol{s_t}, a_t)\n","\\end{align}\n","\n","The code block which describes the linear system and its corresponding quadratic reward function \n","is given below."]},{"cell_type":"code","metadata":{"id":"7Dydj9zIzlIH","executionInfo":{"status":"ok","timestamp":1636110040978,"user_tz":-60,"elapsed":9,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["class QuadReward:\n","    def __init__(self):\n","        self._M = np.array([[10, 0], [0, 1]])\n","        self._Q = np.array([1])\n","\n","    def get_rew(self, s, a):\n","        return -s.T @ self._M @ s - a.T * self._Q * a\n","\n","\n","class LinEnv:\n","    def __init__(self):\n","        self.s_dim = 2\n","        self.a_dim = 1\n","        self.T = 50\n","        self.t = 0\n","        self._A = np.array([[1, 0.1], [0, 0.99]])\n","        self._B = np.array([0, 0.1])\n","        self._s = np.array([2, 1])\n","        self._s_init = np.array([2, 1])\n","        self.s_max = np.ones(2) * 12\n","        self.s_min = -np.ones(2) * 12\n","        self.a_max = 8\n","        self.a_min = -8\n","        self.rew = QuadReward()\n","\n","    def reset(self):\n","        self._s = self._s_init\n","        self.t = 0\n","        return self._s.copy()\n","\n","    def step(self, a):\n","        a_used = np.clip(a, self.a_min, self.a_max)\n","        s_prime = self._A @ self._s + self._B * a_used\n","        r = self.rew.get_rew(self._s, a)\n","        s_prime = np.clip(s_prime, self.s_min, self.s_max)\n","        self._s = np.copy(s_prime)\n","        self.t += 1\n","        return s_prime, r"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uh50zlZxGDfX"},"source":["### **Linear Controller**\n","We consider a linear, stochastic controller of the form\n","\\begin{align}\n","    \\pi(a|\\boldsymbol{s}) = \\mathcal{N}(a|\\boldsymbol{Ks}, \\sigma^2) =\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{1}{2}\\frac{(a-\\boldsymbol{Ks})^2}{\\sigma^2}},\n","\\end{align}\n","where $\\boldsymbol{K} = [k_1, k_2]$ and $\\sigma$ are the learnable parameters. Our learning agents will optimize\n","for those parameters.\n","\n","The following code defines the Linear controller"]},{"cell_type":"code","metadata":{"id":"4oiA8GEdGDfY","executionInfo":{"status":"ok","timestamp":1636110040979,"user_tz":-60,"elapsed":8,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["class LinPolicy:\n","    def __init__(self):\n","        self.s_dim = 2\n","        self.a_dim = 1\n","        self._K = np.zeros(self.s_dim)\n","        self._var = None  # variance, i.e. sigma**2\n","        self._std = None  # standard deviation, i.e. sigma\n","        self.params = np.array([-10, -10, 1])\n","        self.n_params = self.params.shape[0]\n","        self.update_params(self.params)\n","\n","    def update_params(self, params):\n","        self._K[:(self.n_params - self.a_dim)] = params[:-self.a_dim]\n","        self._var = np.diag(params[-self.a_dim:] ** 2)\n","        self._std = np.diag(params[-self.a_dim:])\n","        self.params = params\n","\n","    def get_mean(self, s):\n","        return np.atleast_1d(self._K @ s)\n","\n","    def sample(self, s):\n","        return np.random.multivariate_normal(mean=np.atleast_1d(self.get_mean(s)), \n","                                             cov=np.atleast_2d(self._var))\n","\n","    def grad_log_pi(self, s, a):\n","        grad = np.zeros(self.n_params)\n","        std_inv = 1 / (self._std + 1e-20)\n","        diff = a - self.get_mean(s)\n","        aff_state = s[:(self.n_params - self.a_dim)]\n","        grad_K = diff * aff_state.T * (std_inv ** 2)\n","        grad_sigma = -std_inv + (diff ** 2) * (std_inv ** 3)\n","        grad[:(self.n_params - self.a_dim)] = grad_K\n","        grad[-self.a_dim:] = grad_sigma.squeeze()\n","        return grad"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jAD18wIrGDfZ"},"source":["## **Policy Gradients**\n","Next, we will have a closer look to the individual learning algorithms. Policy Gradient follow the same algorithm scheme, which is shown in the followin **pseudo code**\n","\n","---\n","\n","- **Repeat**  For $k=1, 2, \\dots$\n","    - run policy: sample trajectories {$\\tau_i$} {$i=1,...N$} from $\\pi_{\\boldsymbol{\\theta}}(a|s) $\n","    - Estimate the gradient $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$ : estimate_grad()\n","    - Update the parameters: grad_ascent_step()\n","\n","- **Until convergence**\n","\n","---\n","\n","Based on this general structure, in the following we define a base class **PG**, which contains shared attributes and functions. The function `train` implements the main algorithm loop. Please note that we only change the learning rate and the baseline flags in the following. The parameter `n_it` and `n_traj_samples` stay the same for the algorithms.\n","\n","Also note that for the continuous control problem the discount factor $\\gamma$ is ommited since we set it to 1.\n"]},{"cell_type":"code","metadata":{"id":"J2hc8KALGDfb","executionInfo":{"status":"ok","timestamp":1636110720287,"user_tz":-60,"elapsed":280,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["class PG:\n","    def __init__(self, env, policy, n_it=150, n_traj_samples=25, lr=1e-4, baseline=False):\n","        self.env = env                            # current environment\n","        self.policy = policy                      # current policy object\n","        self.n_it = n_it                          # number of total iterations \n","        self.n_traj_samples = n_traj_samples      # number of traj. samples per iteration\n","        self.lr = lr                              # learning rate\n","        self.baseline = baseline                  # wether to use baseline or not\n","\n","    def estimate_grad(self, rews, state_traj, action_traj):\n","        raise NotImplementedError\n","\n","    def train(self):\n","        \"\"\"\n","        This function will perform the main loop of the policy gradient algorithms.\n","        For plotting and saving purposes, it will return the estimated gradients,\n","        the test rewards and the parameters of each iteration.\n","        :return: tuple of estimated_grads, test_rewards, parameters\n","                estimated_grads: np.ndarray [n_it x n_params]\n","                test_rewards: np.ndarray [n_it]\n","                parameters: np.ndarray[n_it+1 x n_params]\n","        \"\"\"\n","        estimated_grads = np.zeros((self.n_it, self.policy.n_params))\n","        training_rewards = np.zeros(self.n_it)\n","        test_rewards = np.zeros(self.n_it)\n","        parameters = np.zeros((self.n_it + 1, self.policy.n_params)) # +1 means initialization?\n","        parameters[0, :] = self.policy.params.copy()\n","        progress_bar = ProgressBar(num_iterations=self.n_it)\n","        for k in range(self.n_it):\n","            rewards = np.zeros((self.n_traj_samples, self.env.T))\n","            states_trajs = np.zeros((self.n_traj_samples, self.env.T, self.env.s_dim))\n","            action_trajs = np.zeros((self.n_traj_samples, self.env.T, self.env.a_dim))\n","            for j in range(self.n_traj_samples):\n","                s = self.env.reset()\n","                for t in range(self.env.T):\n","                    a = self.policy.sample(s)\n","                    action_trajs[j, t] = a\n","                    states_trajs[j, t] = s\n","                    s, r = self.env.step(a)\n","                    rewards[j, t] = r\n","            grad_estimate = self.estimate_grad(rewards, states_trajs, action_trajs)\n","            estimated_grads[k, :] = grad_estimate\n","            new_params = self.grad_ascent_step(grad_estimate, states_trajs, action_trajs)\n","            parameters[k + 1, :] = new_params\n","            self.policy.update_params(new_params)\n","            training_rewards[k] = np.mean(np.sum(rewards, axis=1))\n","            test_rewards[k] = PG.test_policy(policy=self.policy, env=self.env)\n","            progress_bar(test_reward=test_rewards[k])\n","        return estimated_grads, test_rewards, parameters\n","\n","    def grad_ascent_step(self, grad_estimate: np.ndarray, state_traj=None, action_traj=None):\n","        \"\"\"\n","        This function performs the gradient ascent step.\n","        :param grad_estimate: The estimated gradient of the return: np.ndarray [n_params]\n","        :param state_traj: None (needed for Natural Policy Gradient Update)\n","        :param action_traj: None (needed for Natural Policy Gradient Update)\n","        :return: new parameters: np.ndarray [n_params]\n","        \"\"\"\n","        return self.policy.params + self.lr * grad_estimate\n","\n","    @staticmethod\n","    def test_policy(policy, env, n_trials=10):\n","        use_policy = policy\n","        ep_rewards = np.zeros((n_trials, env.T))\n","        for i in range(n_trials):\n","            s = env.reset()\n","            for t in range(env.T):\n","                a = use_policy.get_mean(s)\n","                s, r = env.step(a)\n","                ep_rewards[i, t] = r\n","        return np.mean(np.sum(ep_rewards, axis=1))"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6k-g4AA1GDfc"},"source":["# **REINFORCE**\n","We start with the most basic **REINFORCE** algorithm. The **pseudocode** is given as\n","\n","---\n","\n","- **Repeat**  For $k=1, 2, \\dots$\n","    - run policy: sample trajectories {$\\tau_i$}$_{i=1,...N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n","    - Estimate the gradient:\n","        - $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N}\\sum_i \\left(\\sum_t\\nabla_{\\boldsymbol{\\theta}}\\log \\pi_\\boldsymbol{\\theta}(a_{i,t}|\\boldsymbol{s}_{i,t})\\right)\\left(\\sum_t\\gamma^tr(\\boldsymbol{s}_{i,t}a_{i,t})\\right)$\n","    - Update the parameters:\n","        - $\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n","\n","- **Until convergence**\n","\n","---\n","\n","The following class inherits from the basic Polciy Gradient class and only needs to override the function\n","'estimate_grad' according to the pseudo code given above. This function estimates the gradient of the return\n","with respect to the parameters of the policy. \n","\n","## **TASK 1: REINFORCE** (3 Points)\n","Your task is to implement the `estimate_grad` function according to the pseudo code shown above (**Don't implement the policy update!**). Through the class attribute `baseline` we can choose if we would like to estimate the gradient with or without a baseline. \n","Make sure that both options are working in your code. \n","Use the following baseline\n","\\begin{align}\n","    b = \\frac{1}{N}\\sum_i\\sum_tr(\\boldsymbol{s}_{i,t}, a_{i,t})\n","\\end{align}\n","\n","After you completed the implementation, run the **Execute REINFORCE** cell. This cell will save three different figures which you will need to submit.\n","\n","*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"qPa2cMF-GDfe","executionInfo":{"status":"error","timestamp":1636110041371,"user_tz":-60,"elapsed":52,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}},"outputId":"ca3bc43b-c3be-4316-ae10-0073a709d1af"},"source":["class REINFORCE(PG):\n","    def estimate_grad(self, rews, state_traj, action_traj):\n","        \"\"\"\n","        This function returns the gradient estimate of the return.\n","        :param rews: all training rewards of the last iteration: np.ndarray \n","                     [n_traj_samples x T], where T is the horizon length\n","        :param state_traj: all states of the last iteration: np.ndarray \n","                     [n_traj_samples x T x s_dim], where T is the horizon length \n","                     and s_dim is the state dimension\n","        :param action_traj: all taken actions of the last iteration: np.ndarray \n","                     [n_traj_samples x T x a_dim], where T is the horizon length \n","                     and a_dim is the action dimension\n","        :return: grad_estimate: the estimated gradient: np.ndarray [n_params]\n","        \"\"\"\n","        ####### TODO ######\n","        \n","        if self.baseline:\n","            baseline = .....\n","        else:\n","            baseline = 0\n","        return 0"],"execution_count":8,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-d2cba362f349>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    baseline = .....\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"JT6Y3xmmGDff"},"source":["### **Execute REINFORCE**\n","Given that **REINFORCE** can be executed with and without baseline, we can now compare both versions of the algorithm. The following cell will executed both versions and plot the rewards, gradient estimates as well as the\n","update history of the parameters $k_1$ and $k_2$ on the contour plot of the return depending on the policy parameters.\n","\n","If you have implemented the algorithm correctly, you should clearly see that the baseline approach is propperly converging and the version without the baseline does not manage to solve the problem.\n","\n","*Hint: The hyperparameters are already tuned. You do not need to tune them.* "]},{"cell_type":"code","metadata":{"id":"gYbVb8znGDfg","executionInfo":{"status":"aborted","timestamp":1636110041345,"user_tz":-60,"elapsed":22,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["# this will create new plt figures \n","fig_dict = create_fig_dict()\n","fig_dict[\"Contour Plot\"] = do_contour_plot(LinEnv())\n","\n","\n","def execute_reinforce(fig_dict):\n","    np.random.seed(0)\n","\n","    reinforce = REINFORCE(env=LinEnv(), policy=LinPolicy(), lr=1e-6, baseline=False)\n","    grads_reinfocre, test_rewards_reinforce, parameters_reinforce = reinforce.train()\n","    finish_training(grads_reinfocre, parameters_reinforce, test_rewards_reinforce, color='cornflowerblue',\n","                    fig_dict=fig_dict,\n","                    legend_label='REINFORCE')\n","\n","    \n","    np.random.seed(0)\n","    reinforce_w_baseline = REINFORCE(env=LinEnv(), policy=LinPolicy(), lr=1e-3, baseline=True)\n","    grads_reinforce_w_baseline, test_rewards_reinforce_w_baseline, parameters_reinforce_w_baseline = reinforce_w_baseline.train()\n","    finish_training(grads_reinforce_w_baseline, parameters_reinforce_w_baseline, test_rewards_reinforce_w_baseline,\n","                    color='blue', fig_dict=fig_dict, legend_label='REINFORCE_WITH_BASELINE')\n","\n","    return fig_dict\n","\n","\n","fig_dict = execute_reinforce(fig_dict)\n","save_plots_from_fig_dict(fig_dict, name='reinforce')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P5t2D9eWGDfh"},"source":["# **Policy Gradient Theorem**\n","\n","The basic version of **REINFORCE** suffers from high variance in the gradient estimates. One fix to this high variance is to exploit the temporal structure in the reward estimate. This leads to use the Monte-Carlo estimate of the reward to come (Q-function) instead of the return. The pseudo code is given as\n","\n","---\n","\n","- **Repeat**  For $k=1, 2, \\dots$\n","    - run policy: sample trajectories {$\\tau_i$}$_{i=1,...N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n","    - Estimate the gradient:\n","        - $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N}\\sum_i \\sum_t\\nabla_{\\boldsymbol{\\theta}}\\log \\pi_\\boldsymbol{\\theta}(a_{i,t}|\\boldsymbol{s}_{i,t})\\left(\\sum_{k=t}\\gamma^{k-t}r(\\boldsymbol{s}_{i,k}a_{i,k}\\right)$\n","    - Update the parameters:\n","        - $\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n","\n","- **Until convergence**\n","\n","---\n","\n","The following class inherits from the basic Polciy Gradient class and only needs to override the function\n","`estimate_grad` according to the pseudo code given above. This function estimates the gradient of the return\n","with respect to the parameters of the policy. \n","\n","## **TASK 2: Policy Gradient Theorem** (3 Points)\n","Your task is to implement the *'estimate_grad'* function according to the pseudo code shown above (**Don't implement the policy update!**). Through the class attribute `baseline` we can choose if we would like to estimate the gradient with or without a baseline. \n","Make sure that both options are working in your code. \n","\n","Use the following time-dependent baseline when you are in time-step *t*\n","\\begin{align}\n","    b_t = \\frac{1}{N}\\sum_i\\sum_{k=t}r(\\boldsymbol{s}_{i,k}, a_{i,k})\n","\\end{align}\n","\n","After you completed the implementation, run the **Execute Policy Gradient Theorem** cell. This cell will save three different figures which you will need to submit.\n","\n","*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*"]},{"cell_type":"code","metadata":{"id":"-We3DjdtGDfi","executionInfo":{"status":"aborted","timestamp":1636110041349,"user_tz":-60,"elapsed":25,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["class PGTheorem(PG):\n","\n","    def estimate_grad(self, rews, state_traj, action_traj):\n","        \"\"\"\n","        This function returns the gradient estimate of the return.\n","        :param rews: all training rewards of the last iteration: np.ndarray \n","                     [n_traj_samples x T], where T is the horizon length\n","        :param state_traj: all states of the last iteration: np.ndarray \n","                     [n_traj_samples x T x s_dim],where T is the horizon length \n","                     and s_dim is the state dimension\n","        :param action_traj: all taken actions of the last iteration: np.ndarray \n","                     [n_traj_samples x T x a_dim], where T is the horizon length \n","                     and a_dim is the action dimension\n","        :return: grad_estimate: the estimated gradient: np.ndarray [n_params]\n","        \"\"\"\n","        ####### TODO ######\n","        # Hint: You might need to use if self.baseline: ..... in order to \n","        #       to distinguish wether to use or not use a baseline\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rSa6VhPRGDfj"},"source":["### **Execute Policy Gradient Theorem**\n","Given that **REINFORCE** with the **Polciy Gradient Theorem** extension can be executed with and without baseline, we can now compare both versions of the algorithm. The following cell will executed both versions and plot the rewards, gradient estimates as well as the\n","update history of the parameters $k_1$ and $k_2$ on the contour plot of the return depending on the policy parameters.\n","\n","If you have implemented the algorithm correctly, you should clearly see that the baseline approach is propperly converging and the version without the baseline is very slow and ends in a very low performance quality.\n","\n","*Hint: The hyperparameters are already tuned. You do not need to tune them.* "]},{"cell_type":"code","metadata":{"id":"iywBqJEyGDfk","executionInfo":{"status":"aborted","timestamp":1636110041351,"user_tz":-60,"elapsed":26,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["fig_dict = create_fig_dict()\n","fig_dict[\"Contour Plot\"] = do_contour_plot(LinEnv())\n","\n","\n","def execute_pgt(fig_dict):\n","    np.random.seed(0)\n","\n","    pgt = PGTheorem(env=LinEnv(), policy=LinPolicy(), lr=1e-5, baseline=False)\n","    grads_pgt, test_rewards_pgt, parameters_pgt = pgt.train()\n","    finish_training(grads_pgt, parameters_pgt, test_rewards_pgt, color='olive', fig_dict=fig_dict,\n","                    legend_label='PGTheorem')\n","\n","    np.random.seed(0)\n","    pgt_with_baseline = PGTheorem(env=LinEnv(), policy=LinPolicy(), lr=1e-3, baseline=True)\n","    grads_pgt_with_baseline, test_rewards_pgt_with_baseline, parameters_pgt_with_baseline = pgt_with_baseline.train()\n","    finish_training(grads_pgt_with_baseline, parameters_pgt_with_baseline, test_rewards_pgt_with_baseline,\n","                    color='green', fig_dict=fig_dict, legend_label='PGTheorem with baseline')\n","    return fig_dict\n","\n","\n","fig_dict = execute_pgt(fig_dict)\n","save_plots_from_fig_dict(fig_dict, name='pgt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OUgeqHdCGDfk"},"source":["# **Natural Policy Gradient**"]},{"cell_type":"markdown","metadata":{"id":"BleImO9bGDfl"},"source":["A common approach in policy search is to apply a trust region constraint, where the policy update is bounded. It has been shown that trust regions highly stabilize the learning process. \n","\n","A trust region approach which can be applied to continous control problems with parametric policy distributions is **Natural Policy Gradient**. Here, the KL-constraint is approximated with the second order Taylor approximation, which results in the **Fisher Information** matrix **F**.\n","\n","Concretely, we can easily calculate **F** as \n","\\begin{align}\n","    \\boldsymbol{F} = \\frac{1}{TN}\\sum_i^N\\sum_t^T \\left[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{i,t}|s_{i,t})\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{i,t}|s_{i,t})^T\\right].\n","\\end{align}\n","\n","The policy's parameter update is then given as\n","\\begin{align}\n","    \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}),\n","\\end{align}\n","where for the gradient estimate $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$ standard techniques like the policy gradient theorem is used.\n","\n","In Natural Policy Gradient, the learning rate parameter $\\alpha ~~(\\eta^{-1}\\text{ in the slides})$ can be solved in closed-form by finding the optimal solution of the dual function to the according Lagrangian (see Task 4). More specifically, $\\alpha$ is given as \n","\\begin{align}\n","    \\alpha = \\sqrt{\\frac{4\\epsilon}{\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})^{T}\\boldsymbol{F}^{-1}\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})}},\n","\\end{align}\n","where $\\epsilon$ is the hyperparameter, bounding the expected KL between the new and the old policy.\n","\n","Since we will use the gradient estimate from the **Policy Gradient Theorem**, the following class inherits from the **PGTheorem** class, i.e. you need to have properly solved Task 2 in order to be able to solve this task.\n","\n","\n","## **TASK 3: Natural Policy Gradient** (4 Points)\n","\n","- Implement the function `get_F`, which will return the Fisher information matrix given state-action trajectories. Apply the equations mentioned above. \n","- Implement the function `grad_ascent_step`, which calculates and returns the new parameters of the policy according to the update rule mentioned above. When implementing the update, please also use the closed-form solution to $\\alpha$ to scale the update. We will use $\\epsilon=0.5$.\n","\n","*Note: You will need to have properly solved TASK 2 in order to be able to solve this task.*\n","\n","After you completed the implementation, run the **Natural Policy Gradient** cell. This cell will save three different figures which you will need to submit.\n","\n","*Hint: To get the gradient of the log policy for current state $\\boldsymbol{s}_{i,t}$ and current action $a_{i,t}$ from the rollout i at time step t, you will need to call `self.policy.grad_log_pi(current_state, current_action)`, where current_state is $\\boldsymbol{s}_{i,t}$ and current action is $a_{i,t}$.*"]},{"cell_type":"code","metadata":{"id":"3lv9x-ChGDfm","executionInfo":{"status":"aborted","timestamp":1636110041353,"user_tz":-60,"elapsed":28,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["class NPG(PGTheorem):\n","    def __init__(self, env, policy, n_it=150, n_traj_samples=25, eps=0.5, baseline=True):\n","        super(NPG, self).__init__(env, policy, n_it=n_it, n_traj_samples=n_traj_samples, \n","                                  lr=None, baseline=baseline)\n","        self.eps = eps\n","        \n","    def get_F(self, state_traj, action_traj):\n","        \"\"\"\n","        This function calculates the Fisher Information matrix.\n","        :param state_traj: all states of the last iteration: np.ndarray [n_traj_samples x T x s_dim],\n","                     where T is the horizon length and s_dim is the state dimension\n","        :param action_traj: all taken actions of the last iteration: np.ndarray [n_traj_samples x T x a_dim],\n","                     where T is the horizon length and a_dim is the action dimension\n","        :return: F: returns the Fisher information matrix: np.ndarray [n_params x n_params]\n","        \"\"\"\n","        ####### TODO ######\n","        # implement here the sample based Fisher information matrix F described in the \n","        # equations above\n","        return 0\n","\n","    def grad_ascent_step(self, grad_estimate, state_traj, action_traj):\n","        \"\"\"\n","        Performs an updated on the parameters of the policy according to the Natural Policy Gradient Rule by using\n","        the current gradient estimate (grad_estimate) of the Policy Gradient theorem with baseline, the\n","        state trajectories and the action trajectories.\n","        :param grad_estimate: np.ndarray [n_params]\n","        :param state_traj: np.ndarray [n_traj_samples x T x s_dim]\n","        :param action_traj: np.ndarray [n_traj_samples x T x a_dim]\n","        :return: updated policy parameters: np.ndarray [n_params]\n","        \"\"\"\n","        ####### TODO ######\n","        # implement here the gradient update of the parameters described in the equations\n","        # above. Also implement the optimal step size parameter alpha described in the \n","        # equations above.\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3xQw8XXxGDfn"},"source":["### **Execute Natural Policy Gradient**\n","The following cell will executed the natural policy gradient implementation which uses the Policy gradient Theorem with baseline to estimate the vanilla gradient and plot the rewards, gradient estimates as well as the\n","update history of the parameters $k_1$ and $k_2$ on the contour plot of the return depending on the policy parameters.\n","\n","*Hint: The hyperparameters are already tuned. You do not need to tune them.* "]},{"cell_type":"code","metadata":{"id":"lOQZBwUOGDfn","executionInfo":{"status":"aborted","timestamp":1636110041355,"user_tz":-60,"elapsed":30,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["\n","fig_dict = create_fig_dict()\n","fig_dict[\"Contour Plot\"] = do_contour_plot(LinEnv())\n","\n","\n","def execute_natural_pg(fig_dict):\n","    np.random.seed(0)\n","    npg = NPG(env=LinEnv(), policy=LinPolicy(), eps=0.5, baseline=True)\n","    grads_npg, test_rewards_npg, parameters_npg = npg.train()\n","    finish_training(grads_npg, parameters_npg, test_rewards_npg, color='orange', fig_dict=fig_dict,\n","                    legend_label='NPG')\n","    return fig_dict\n","\n","\n","fig_dict = execute_natural_pg(fig_dict)\n","save_plots_from_fig_dict(fig_dict, name='npg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sa9WQaENGDfo"},"source":["\n","\n","**The following cell will execute all algorithms and plot the results into one plot. It will furthermore save the figures to your google drive. You will need to submit the plots together with the ipython notebook in a compressed file.**\n"]},{"cell_type":"code","metadata":{"id":"vQo7G6f6GDfo","executionInfo":{"status":"aborted","timestamp":1636110041356,"user_tz":-60,"elapsed":31,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["fig_dict = create_fig_dict()\n","fig_dict[\"Contour Plot\"] = do_contour_plot(LinEnv())\n","fig_dict = execute_reinforce(fig_dict)\n","fig_dict = execute_pgt(fig_dict)\n","fig_dict = execute_natural_pg(fig_dict)\n","save_plots_from_fig_dict(fig_dict, name='all')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wz5mvxdGDfp"},"source":["## **Task 4: Natural Policy Gradient Step Size** (4 Points)\n","\n","Recall that Natural Gradients use a Taylor approximation of the trust region problem, i.e., the objective is given as \n","$$\\boldsymbol{g}^* = \\underset{\\boldsymbol{g}}{\\textrm{argmax}} ~~ \\boldsymbol{g}^T\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} ~~ \\textrm{s.t.} ~~ \\boldsymbol{g}^T \\boldsymbol{F} \\boldsymbol{g} \\leq \\epsilon.$$\n","By introducing a Lagrangian multiplier $\\eta$ we can construct the corresponding Lagrangian\n","$$ L(\\boldsymbol{g}, \\eta) = \\boldsymbol{g}^T \\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{J} + \\eta \\left(\\epsilon - \\boldsymbol{g}^T\\boldsymbol{F}\\boldsymbol{g}\\right).$$\n","\n","**Exercise:** Derive $\\boldsymbol{g}^*$. Also solve the dual, your solution should not depend on $\\eta$ anymore! You can use the markdown cell below to answer this task.\n","\n","*Note:You can also submit any other file format (photo, pdf, ...) as long as we are able to identify your solution.*\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EAy5u7UMGDfp"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"RlbajFwXGDfq"},"source":["## **Discrete Control with Deep Neurel Net Controller**"]},{"cell_type":"markdown","metadata":{"id":"LFWSh65aGDfq"},"source":["Next, we will consider training Deep Neural Net policies to solve the discrete pole balancing environment called 'CartPole'. We will use the 'CartPole-v1' environment from OpenAI Gym, which you can find here:\n","\n","https://gym.openai.com/envs/CartPole-v1/. \n","\n","**Policy gradients With Neural Network Policies.**\n","\n","We will use the **REINFORCE** algorithm **with baselines** and **policy gradient theorem** as shown below. \n","\n","---\n","\n","- **Repeat**  For $k=1, 2, \\dots$\n","    - run policy: sample trajectories {$\\tau_i$}$_{i=1,...N}$ from $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$\n","    - Estimate the gradient:\n","        - $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\approx \\frac{1}{N}\\sum_i \\left(\\sum_t\\nabla_{\\boldsymbol{\\theta}}\\log \\pi_\\boldsymbol{\\theta}(a_{i,t}|\\boldsymbol{s}_{i,t})\\right)\\left(Q_t-b_t\\right)$\n","    - Update the parameters:\n","        - $\\boldsymbol{\\theta}\\leftarrow \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$\n","\n","- **Until convergence**\n","\n","---\n","$Q_t$ is the Q-value at time t, $Q^{\\pi}(s_t, a_t)$, and $b_t$ is a baseline.\n","The baseline is given as\n","\\begin{align}\n","    b_t = \\frac{1}{N}\\sum_i\\sum_{k=t}r(\\boldsymbol{s}_{i,k}, a_{i,k}).\n","\\end{align}\n","However we will now replace the linear policies with deep neural networks and compute policy gradients with automatic differentiation. To do this we create a graph in such a way that its gradient is the policy gradient. \n","\n","We first import the necessary packages."]},{"cell_type":"code","metadata":{"id":"uRyBdYLsGDfr","executionInfo":{"status":"aborted","timestamp":1636110041360,"user_tz":-60,"elapsed":35,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.distributions as distributions\n","import gym\n","\n","from typing import Tuple"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRptFLzDGDfr","executionInfo":{"status":"aborted","timestamp":1636110041361,"user_tz":-60,"elapsed":35,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["train_env = gym.make('CartPole-v1')\n","test_env = gym.make('CartPole-v1')\n","\n","SEED = 1234\n","\n","train_env.seed(SEED);\n","test_env.seed(SEED+1);\n","np.random.seed(SEED);\n","torch.manual_seed(SEED);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCKuGKGyGDfs"},"source":["### **Building Neural Network in PyTorch** \n","    \n","This part of the code creates a feed forward neural network using pytorch library, which will form the policy  $\\pi_{\\boldsymbol{\\theta}}(a|\\boldsymbol{s})$.\n","\n","We use pytorch modules torch.nn.Linear and torch.nn.ReLU() to do so."]},{"cell_type":"code","metadata":{"id":"wW3JbfxKGDfs","executionInfo":{"status":"aborted","timestamp":1636110041363,"user_tz":-60,"elapsed":37,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["class MLP(nn.Module):\n","    def __init__(self, state_dim: int, act_dim: int, hidden_units=[60,60,60]):\n","        \"\"\"\n","        :param state_dim: dimension of the state space\n","        :param state_dim: dimension of the actions\n","        :param hidden_units: list of integers corresponding to hidden units\n","        \"\"\"\n","        super(MLP, self).__init__()\n","\n","        self._state_dim = state_dim\n","        self._act_dim = act_dim\n","        self._hidden_units = hidden_units\n","        # Define network\n","        layers = []\n","        last_hidden = self._state_dim\n","        # hidden layers\n","        for hidden_dim in self._hidden_units:\n","            layers.append(nn.Linear(in_features=last_hidden, out_features=hidden_dim))\n","            layers.append(nn.ReLU())\n","            last_hidden = hidden_dim\n","        self._hidden_layers, size_last_hidden = nn.ModuleList(layers), last_hidden\n","\n","\n","        self._mean_layer = nn.Linear(in_features=size_last_hidden, out_features=self._act_dim)\n","        \n","\n","    def forward(self, input: torch.Tensor) \\\n","            -> Tuple[torch.Tensor]:\n","        \"\"\" forward pass of decoder\n","        :param input:\n","        :return: output mean\n","        \"\"\"\n","        h = input\n","        for layer in self._hidden_layers:\n","            h = layer(h)\n","\n","        mean = self._mean_layer(h)\n","\n","\n","        return mean"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWHDOMWvGDft"},"source":["### **Computing Q-values**\n","The code block computes numpy arrays for Q-values which will be used to compute advantages.\n","\n","Recall that the expression for the policy gradient PG is\n","\n","\\begin{align}\n","    J = \\text{E}_{\\tau\\sim p(\\tau)}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{\\theta}}\\log \\pi(a_t|s_t)(Q_t-b_t)\\right],\n","\\end{align}\n","where $ \\tau=(s_0, a_0, ...)$ is a trajectory, $Q_t$ is the Q-value at time $t$ and $b_t$ is a baseline.\n","\n","\n","  We can obtain four different cases, controlled by the flag 'pg_theorem' and 'baselines' flag:\n","\n","**Case 1: trajectory-based Policy Gradients (vanilla REINFORCE)**\n","**(pg_theorem = False)**\n","\n","Instead of $Q^{\\pi}(s_t, a_t)$, we use the total discounted reward summed over\n","entire trajectory (regardless of which time step the Q-value should be for).\n","For this case, the policy gradient estimator is $ \\text{E}_{\\tau\\sim p(\\tau)}\\left[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi(a_t|s_t) \\cdot \\text{R}(\\tau)\\right]$,  where $ \\text{R}(\\tau) = \\sum_{k=0}^T \\gamma^{k} r_{k} $.\n","\n","**Case 2: PG Theorem applied**\n","**(pg_theroem = True)**\n","\n","Here, you estimate $Q^{\\pi}(s_t, a_t)$ by the discounted sum of rewards starting from time step t. Thus, you should compute $Q_t = \\sum_{k=t}^T \\gamma^{(k-t)} r_{k}$\n","\n","**Case 3: No baselines**\n","**(baselines = False)**\n","\n","Here we set b_t, the baselines to be 0. This is the 'vanilla' PG without baselines.\n","\n"," \\begin{align}\n","    PG =\\text{E}_{\\tau\\sim p(\\tau)} \\sum_{t=0}^T \\nabla_{\\boldsymbol{\\theta}} \\log \\pi(a_t|s_t)  (Q_t )\n","\\end{align}\n","\n","**Case 4: PG with baselines**\n","**(baselines = True)**\n","\n","Here we use one of the baselines discussed in the lecture. Simplest one being the mean of the rewards.\n","\n"," \\begin{align}\n","    PG =\\text{E}_{\\tau\\sim p(\\tau)} \\sum_{t=0}^T \\nabla_{\\boldsymbol{\\theta}} \\log \\pi(a_t|s_t)  (Q_t - b_t )\n","\\end{align}\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"KKQ8r0yrGDfu","executionInfo":{"status":"aborted","timestamp":1636110041365,"user_tz":-60,"elapsed":39,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def calculate_returns(rewards, discount_factor, baselines, pg_theorem, device):\n","    \n","    returns = []\n","    R = 0\n","    \n","    if pg_theorem:\n","      for r in reversed(rewards):\n","          R = r + R * discount_factor\n","          returns.insert(0, R)\n","    else:\n","      for r in reversed(rewards):\n","        R = r + R * discount_factor\n","      for r in reversed(rewards):\n","        returns.insert(0, R)\n","        \n","    returns = torch.tensor(returns).to(device)\n","    \n","    if baselines:\n","        returns = (returns - returns.mean()) \n","        \n","    return returns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mHVmFm5GDfu"},"source":["## **Task 5: Computing Loss** (1 Points)\n","\n","Here we create a \"pseudo loss\" which is the weighted maximum likelihood, $\\sum_{t=0}^T \\log \\pi_{\\theta}(a_t|s_t)  (Q_t - b_t )$, using the stored `log_prob_actions` and `returns / Q `values computed in the previous section. The gradient of this loss function with respect to the neural network parameters ($\\theta$) is the policy gradient.\n","\n","After computing the loss, execute all the cells given below. In the end it will plot a reward curve with the given hyperparameters and save the figures to your google drive. You will need to submit the plot (for this task named `Deep_PG_Reward.png`) together with the ipython notebook in a compressed file."]},{"cell_type":"code","metadata":{"id":"sfHfHliIGDfv","executionInfo":{"status":"aborted","timestamp":1636110041366,"user_tz":-60,"elapsed":40,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def update_policy(returns, log_prob_actions, optimizer):\n","    \"\"\"\n","    This function calculates the loss and backpropagate the errors using pytorch optmizers.\n","    :param returns: all the rewards recieved from the previous trajectory: torch.Tensor \n","                 [T], where T is the horizon length\n","    :param log_prob_actions: all the log probabilities of the policy from the previous \n","                             trajectory torch.Tensor [T],where T is the horizon length.\n","    :param optimizer: the torch optimizer object https://pytorch.org/docs/stable/optim.html\n","    :return: updated loss value: torch.Tensor [1], of float type \n","    \"\"\"\n","    \n","    returns = returns.detach()\n","    \n","    ##### TODO: implement here the loss described in the Task\n","    loss = .....\n","    \n","    ## Backpropagate using pytorch autograd. We will use the Adam Optimizer to do this, though any optimizer\n","    ## would work in practice.\n","    optimizer.zero_grad()\n","    \n","    loss.backward()\n","    \n","    optimizer.step()\n","    \n","    return loss.item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NecpGvCGDfv"},"source":["### **Training the network by sampling discrete actions**\n","\n","Here we provide the code for training and evaluation loop. "]},{"cell_type":"code","metadata":{"id":"Vc8zol0JGDfw","executionInfo":{"status":"aborted","timestamp":1636110041367,"user_tz":-60,"elapsed":40,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def train(env, policy, optimizer, discount_factor, baselines, pg_theorem, device):\n","    \n","    policy.train()\n","    \n","    log_prob_actions = []\n","    rewards = []\n","    done = False\n","    episode_reward = 0\n","\n","    state = env.reset()\n","\n","    while not done:\n","\n","        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n","\n","        action_pred = policy(state)\n","        \n","        action_prob = F.softmax(action_pred, dim = -1)\n","        \n","        ## Here we use torch.distributions to sample discrete actions from a categorical\n","        ## distribution. https://pytorch.org/docs/stable/distributions.html\n","        dist = distributions.Categorical(action_prob)\n","\n","        action = dist.sample()\n","        \n","        ## Here we use torch.distributions to calculate the log probability.\n","        ## https://pytorch.org/docs/stable/distributions.html\n","        log_prob_action = dist.log_prob(action)\n","        \n","        state, reward, done, _ = env.step(action.item())\n","\n","        log_prob_actions.append(log_prob_action)\n","        rewards.append(reward)\n","\n","        episode_reward += reward\n","\n","    log_prob_actions = torch.cat(log_prob_actions)\n","        \n","    returns = calculate_returns(rewards, discount_factor, baselines, pg_theorem, device)\n","\n","    loss = update_policy(returns, log_prob_actions, optimizer)\n","\n","    return loss, episode_reward\n","\n","def evaluate(env, policy, device):\n","    \n","    policy.eval()\n","    \n","    done = False\n","    episode_reward = 0\n","\n","    state = env.reset()\n","\n","    while not done:\n","        \n","        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n","        \n","        with torch.no_grad():\n","        \n","            action_pred = policy(state)\n","            \n","            action_prob = F.softmax(action_pred, dim = -1)\n","                            \n","        ## Here we select the action with the highest probability.\n","        action = torch.argmax(action_prob, dim = -1)\n","\n","        state, reward, done, _ = env.step(action.item())\n","\n","        episode_reward += reward\n","        \n","    return episode_reward"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pk_hYPcbGDfw"},"source":["### **Tuning Hyperparameters and plotting reward curve.**\n","\n","Here we run the algorithms for 5 times and report the reward curve with mean value and error bounds.\n","\n","You can play around with the hyperparameters and see how each of these 4 affect the algorithms performance. However, please submit the saved figures with the default parameters given here."]},{"cell_type":"code","metadata":{"id":"0ja3Pjt3GDfx","executionInfo":{"status":"aborted","timestamp":1636110041369,"user_tz":-60,"elapsed":42,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["##Hyperparameters\n","hidden_units = [30,15]\n","baselines = True\n","pg_theorem = True\n","learning_rate = 5e-3\n","\n","\n","## Run the experiments 5 times\n","max_episodes = 300\n","discount_factor = 0.99\n","input_dim = train_env.observation_space.shape[0]\n","output_dim = train_env.action_space.n\n","\n","device = torch.device('cuda')\n","\n","n_runs = 5\n","train_rewards = torch.zeros(n_runs, max_episodes)\n","test_rewards = torch.zeros(n_runs, max_episodes)\n","\n","for run in range(n_runs):\n","    \n","    policy = MLP(input_dim, output_dim, hidden_units)\n","    policy = policy.to(device)\n","    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n","    \n","    for episode in tqdm.tqdm(range(max_episodes), desc=f'Run: {run}'):\n","        \n","        loss, train_reward = train(train_env, policy, optimizer, discount_factor, baselines, pg_theorem, \n","                                   device)\n","        \n","        test_reward = evaluate(test_env, policy, device)\n","        \n","        train_rewards[run][episode] = train_reward\n","        test_rewards[run][episode] = test_reward\n","        \n","        \n","## Plot the Reward Curves        \n","idxs = range(max_episodes)\n","fig, ax = plt.subplots(1, figsize=(10,6))\n","ax.plot(idxs, test_rewards.mean(0))\n","ax.fill_between(idxs, test_rewards.min(0).values, test_rewards.max(0).values, alpha=0.1)\n","ax.set_xlabel('Steps')\n","ax.set_ylabel('Rewards');\n","save_figure('Deep_PG_Reward')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jb--o6G6GDfy","executionInfo":{"status":"aborted","timestamp":1636110041370,"user_tz":-60,"elapsed":43,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":[""],"execution_count":null,"outputs":[]}]}