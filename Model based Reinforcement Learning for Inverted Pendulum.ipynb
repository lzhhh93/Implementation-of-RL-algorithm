{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"6_MBRL_Solutions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H09zLzLtLSw1"},"source":["#Homework 6: Model Based RL With Deep Dynamics Models (10 Pts)\n","\n","All homeworks are self-contained. They can be completed in their respective notebooks. To edit and re-run code, you can therefore simply edit and restart the code cells below. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). This file should automatically be synced with your Google Drive. We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout. However, you will need to re-mount your Google Drive and re-install packages with every new instance."]},{"cell_type":"code","metadata":{"id":"JDp0AADgLU6A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e90931ee-547d-4c7b-b2fc-873262462e7e","executionInfo":{"status":"ok","timestamp":1639125516134,"user_tz":-60,"elapsed":1760,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["# Your work will be stored in a folder called `drl_ws21` by default to prevent Colab\n","# instance timeouts from deleting your edits.\n","# We do this by mounting your google drive on the virtual machine created in this colab\n","# session. For this, you will likely need to sign in to your Google account and copy a\n","# passcode into a field below\n","\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"qaGhwNOxLgqC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"318dea1a-1c6c-446d-eaeb-e91cb6831ebe","executionInfo":{"status":"ok","timestamp":1639125516134,"user_tz":-60,"elapsed":3,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["# Create paths in your google drive\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/drl_ws21'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","    % mkdir $DRIVE_PATH\n","\n","# the space in `My Drive` causes some issues,\n","# make a symlink to avoid this\n","SYM_PATH = '/content/drl_ws21'\n","if not os.path.exists(SYM_PATH):\n","    !ln -s $DRIVE_PATH $SYM_PATH\n","% cd $SYM_PATH"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/drl_ws21\n"]}]},{"cell_type":"code","metadata":{"id":"9JCctqJjLnnP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6379ceb5-801e-4d9a-f876-f3bbec213dbe","executionInfo":{"status":"ok","timestamp":1639125529715,"user_tz":-60,"elapsed":13583,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["# Install **python** packages\n","\n","!pip install matplotlib numpy tqdm torch pybullet\n","# for open ai gym\n","!pip install gym\n","\n","!git clone https://github.com/benelot/pybullet-gym lib/pybullet-gym\n","!pip install -e lib/pybullet-gym"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n","Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","fatal: destination path 'lib/pybullet-gym' already exists and is not an empty directory.\n","Obtaining file:///content/gdrive/My%20Drive/drl_ws21/lib/pybullet-gym\n","Requirement already satisfied: pybullet>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from pybulletgym==0.1) (3.2.1)\n","Installing collected packages: pybulletgym\n","  Attempting uninstall: pybulletgym\n","    Found existing installation: pybulletgym 0.1\n","    Can't uninstall 'pybulletgym'. No files were found to uninstall.\n","  Running setup.py develop for pybulletgym\n","Successfully installed pybulletgym-0.1\n"]}]},{"cell_type":"code","metadata":{"id":"430EKj9cm5ZK","executionInfo":{"status":"ok","timestamp":1639125534867,"user_tz":-60,"elapsed":5160,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tkQ7Vo5914o9"},"source":["Restart your runtime once the packages are Installed."]},{"cell_type":"markdown","metadata":{"id":"Dxzj7M9JMtBD"},"source":["We start by importing some necessary packages and defining helper functions"]},{"cell_type":"code","metadata":{"id":"3L8vmmOmMv5A","executionInfo":{"status":"ok","timestamp":1639125541484,"user_tz":-60,"elapsed":6619,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython import display\n","import pybulletgym\n","from gym.wrappers import Monitor\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import functional as F\n","\n","from tqdm import tqdm\n","import datetime\n","import time\n","\n","import gym\n","import numpy as np\n","import seaborn as sns\n","\n","# specify the path to save the recordings of this run to.\n","data_path = '/content/drl_ws21/exercise_6'\n","data_path = os.path.join(data_path, time.strftime(\"%d-%m-%Y_%H-%M\"))\n","if not (os.path.exists(data_path)):\n","    os.makedirs(data_path)\n","\n","\n","# this function will automatically save your figure into your google drive folder (if correctly mounted!)\n","def save_figure(save_name: str) -> None:\n","    assert save_name is not None, \"Need to provide a filename to save to\"\n","    plt.savefig(os.path.join(data_path, save_name + \".png\"))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Czwxn0NSNJaV"},"source":["#Set Up The Pybullet Inverted Pendulum Environment\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WycKBzg_NvOz"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"code","metadata":{"id":"UlrqeHoCmlY0","executionInfo":{"status":"ok","timestamp":1639125541806,"user_tz":-60,"elapsed":330,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["from gym.wrappers import Monitor\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment \n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"XqNF_x0cwyzv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a004b08-3a79-469e-f24b-b1cda63d26e1","executionInfo":{"status":"ok","timestamp":1639125544028,"user_tz":-60,"elapsed":2027,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":[" env = gym.make('InvertedPendulumSwingupPyBulletEnv-v0')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]}]},{"cell_type":"markdown","metadata":{"id":"a2hjempk37JG"},"source":["# Model-Based Reinforcement Learning\n","\n","## Principle\n","We consider the optimal control problem of an MDP with a **unknown deterministic** reward function $R$ and subject to **unknown deterministic** dynamics $s_{t+1} = f(s_t, a_t)$:\n","\n","$$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)$$\n","\n","In **model-based reinforcement learning**, this problem is solved in **two steps**:\n","1. **Model learning**:\n","We learn a model of the dynamics $f_\\theta \\simeq f$ and reward function $r_\\theta \\simeq r$ through regression on interaction data.\n","2. **Planning**:\n","We leverage the dynamics model $f_\\theta$ to compute the optimal trajectory $$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t r_{\\theta}(\\hat{s}_t,a_t)$$ following the learnt dynamics $\\hat{s}_{t+1} = f_\\theta(\\hat{s}_t, a_t)$.\n","\n","(We can easily extend to stochastic dynamics, but we consider the simpler case in this homework)\n","\n","\n","In this homework you will implement the model-based algorithm proposed in section IV of [this paper](https://arxiv.org/abs/1708.02596) with some differences: \n","\n","1. along with the next environment state, also the reward is learned. To do that another neural network has been used.\n","2. We train on the pybullet gym environment of inverted pendulum.\n","\n","$$\n","\\begin{aligned}\n","&\\overline{\\text { Algorithm } \\mathbf{1} \\text { Model-based Reinforcement Learning }} \\\\\n","&\\hline \\text { 1: gather dataset } \\mathcal{D}_{\\text {RAND }} \\text { of random trajectories } \\\\\n","&\\text { 2: initialize empty dataset } \\mathcal{D}_{\\text {RL }} \\text {, and randomly initialize } f_{\\theta} \\\\\n","&  \\text{ 3: } \\textbf{for} \\text{ iter=1 to max_iter } \\textbf{do}  \\\\\n","&\\quad  \\quad \\quad \\textbf {Model Learning } \\\\\n","&\\text { 4:} \\quad  \\quad \\text{ train } f_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text{ and } r_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text { by performing gradient descent on MSE Loss }  \\\\\n","& \\quad \\quad  \\quad \\text { using } \\mathcal{D}_{\\text {RAND }} \\text { and } \\mathcal{D}_{\\text {RL }} \\\\\n","&\\quad  \\quad \\quad \\textbf {Planning and More Experience Collection } \\\\\n","&\\text { 5: } \\quad  \\quad  \\textbf { for } t=1 \\text { to } T \\textbf { do } \\\\\n","&\\text { 6: } \\quad \\quad \\quad \\quad \\text { get agent's current state } \\mathbf{s}_{t} \\\\\n","&\\text { 7: } \\quad \\quad \\quad \\quad \\text { use } f_{\\theta} \\text { and } r_{\\theta} \\text { to estimate optimal action sequence } \\mathbf{A}_{t}^{(H)} \\\\\n","&\\text { 8: } \\quad \\quad \\quad \\quad \\text { execute first action } \\mathbf{a}_{t} \\text { from selected action sequence } \\\\\n","&\\text { 9: } \\quad \\quad \\quad \\quad\\text { Add } \\{s_t,  s_{t+1}, r_t, a_t\\} \\text { to } \\mathcal{D}_{\\text {RL }} \\\\\n","&\\text { 10: } \\quad \\text { end for } \\\\\n","&\\text { 11: end for } \\\\\n","&\\hline\n","\\end{aligned}\n","$$\n","\n","You will notice that compared to model free methods, model based methods are more sample efficient. For this environment expect the algorithm to receive reasonable rewards (greater than 600 in about 15 iterations)."]},{"cell_type":"markdown","metadata":{"id":"4-WRSNNEOX5w"},"source":["# Collect Random Trajectories To Train A Dynamics Model and Reward Model (Experience Collection)\n","\n","First, we randomly interact with the environment to produce a batch of experiences \n","\n","$$D = \\{s_t,  s_{t+1}, r_t, a_t\\}_{t\\in[1,N]}$$"]},{"cell_type":"code","metadata":{"id":"_AymYXhlOocs","executionInfo":{"status":"ok","timestamp":1639125544028,"user_tz":-60,"elapsed":4,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def gather_random_trajectories(num_traj, env):\n","    '''\n","    Run num_traj random trajectories to gather information about the next state and reward.\n","    Data used to train the models in a supervised way.\n","    '''\n","    dataset_random = []\n","\n","    game_rewards = []\n","    with tqdm(total=num_traj, position=0, leave=True) as pbar:\n","        for n in tqdm(range(num_traj),position=0, leave=True):\n","            #env.render()\n","            obs = env.reset()\n","            while True:\n","                sampled_action = env.action_space.sample()\n","                new_obs, reward, done, _ = env.step(sampled_action)\n","\n","\n","                dataset_random.append([obs, new_obs, reward, sampled_action])\n","\n","                obs = new_obs\n","                game_rewards.append(reward)\n","\n","                if done:\n","                    break\n","            pbar.update()\n","\n","    # print some stats\n","    print('Mean R:', np.round(np.sum(game_rewards) / num_traj, 2), 'Max R:', np.round(np.max(game_rewards), 2),\n","          np.round(len(game_rewards) / num_traj))\n","\n","    return dataset_random\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ed5n0DiPGOZ"},"source":["#Deep Dynamics Models With Feed Forward Neural Network\n","\n","A dynamics model takes in the state ($s_t$) and action($a_t$) at the current time step and predicts the state ($s_{t+1}$) at the next time step.  \n","\n","However this function can be difficult to learn when the states $s_t$ and $s_{t+1}$ are too similar and the action has seemingly littl eeffect on the output; this difficulty becomes more pronouncedas the time between states $\\Delta t$ becomes smaller and the state differences do not indicate the underlying dynamics well. Thus we typically a function that predict the differences to the next state. i.e. $\\Delta s_{t+1} = s_{t+1} - s_t = f(s_t,a_t)$. This allows us to learn more accurate models in practice. \n","\n","In the following block, we define the Dynamics Model Class. Each instance of this class is a Neural Dynamics Model that computes the dynamics function $\\Delta s_{t+1} = f_{\\Theta}(s_t, a_t)$. The input of this Network is a state and action at current time step $s_t$, $a_t$ respectively and the output the differences to the next state at the next time step. Please note that for NN training, we normally feed data in a mini-batch manner. Since each state in current task is a 1st order image tensor (concatenation of $s_t$ and $a_t$), the Dynamics model expects input to be a 2nd order tensor with shape (mini-batch, state_dim + action_dim). And the output is a 2nd order tensor with shape (mini-batch, state_dim)."]},{"cell_type":"code","metadata":{"id":"pPtT-9L7O9CK","executionInfo":{"status":"ok","timestamp":1639125544029,"user_tz":-60,"elapsed":4,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["###########################Create A Neural Network Dynmics Model####################################\n","class NNDynamicModel(nn.Module):\n","    '''\n","    Model that predict the differnce to next state, given the current state and action\n","    '''\n","    def __init__(self, input_dim, obs_output_dim):\n","      '''\n","      input_dim: state_dim + action+dim\n","      output_dim: state_dim\n","      '''\n","      super(NNDynamicModel, self).__init__()\n","\n","      self.mlp = nn.Sequential(\n","          nn.Linear(input_dim, 512),\n","          nn.BatchNorm1d(num_features=512),\n","          nn.ReLU(),\n","          nn.Linear(512,256),\n","          nn.BatchNorm1d(num_features=256),\n","          nn.ReLU(),\n","          nn.Linear(256, obs_output_dim)\n","      )\n","\n","    def forward(self, x):\n","        return self.mlp(x.float())"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hI_qEzp1Tmvr"},"source":["#Deep Reward Models With Feed Forward Neural Network\n","\n","A reward model takes in the state ($s_t$) and action($a_t$) at the current time step and predicts reward ($r_{t}$) that we can obtain based on that action.  \n","\n","In the following block, we define the Reward Model Class. Each instance of this class is a Neural Reward Model that computes the reward function $r_{t} = r_{\\theta}(s_t, a_t)$. The input of this Network is a state and action at current time step $s_t$, $a_t$ respectively and the output the rewards at the next time step. Please note that for NN training, we normally feed data in a mini-batch manner. Since each state in current task is a 1st order image tensor (concatenation of $s_t$ and $a_t$), the Dynamics model expects input to be a 2nd order tensor with shape (mini-batch, state_dim + action_dim). And the output is a 2nd order tensor with shape (mini-batch, reward_dim)."]},{"cell_type":"code","metadata":{"id":"aNwMbuFnUTTm","executionInfo":{"status":"ok","timestamp":1639125544029,"user_tz":-60,"elapsed":4,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["class NNRewardModel(nn.Module):\n","    '''\n","    Model that predict the reward given the current state and action\n","    '''\n","    def __init__(self, input_dim, reward_output_dim):\n","        '''\n","        input_dim: state_dim + action+dim\n","        output_dim: reward_dim\n","        '''\n","        super(NNRewardModel, self).__init__()\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.BatchNorm1d(num_features=512),\n","            nn.ReLU(),\n","            nn.Linear(512,256),\n","            nn.BatchNorm1d(num_features=256),\n","            nn.ReLU(),\n","            nn.Linear(256, reward_output_dim)\n","        )\n","\n","    def forward(self, x):\n","        return self.mlp(x.float())"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6WZSisIVspI"},"source":["#Task 1: Train The Dynamics And Reward Models (3 Pts)\n"]},{"cell_type":"markdown","metadata":{"id":"St4dSNkWO28h"},"source":["We can now train our models $f_\\theta$ and $r_\\theta$ in a supervised fashion to minimize an MSE loss over our experience batch by stochastic gradient descent:\n","\n","$$L_{Dynamics} = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||\\Delta s_{t+1}- f_\\theta(s_t, a_t)||^2$$\n","\n","$$L_{Reward} = \\frac{1}{|D|}\\sum_{s_t,a_t,r_{t}\\in D}||r_{t}- r_\\theta(s_t, a_t)||^2$$\n","\n","-----\n","In practice, it’s helpful to normalize the target of a neural network.  So in the code, we’ll train the network to predict a normalized version of the change in state, as in\n","\n","$$L_{Dynamics} = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||\\text{normalize}(\\Delta s_{t+1})- f_\\theta(s_t, a_t)||^2$$\n","\n","Similarly, we’ll train the network to predict a normalized version of the reward, as in\n","\n","$$L_{Reward} = \\frac{1}{|D|}\\sum_{s_t,a_t,r_{t}\\in D}||\\text{normalize}(r_{t})- r_\\theta(s_t, a_t)||^2$$\n","\n","\n","\n","------\n","Since $f_{\\theta}$ is trained to predict the normalized state difference, you generate thenext prediction with\n","$$\n","\\hat{\\mathbf{s}}_{t+1}=\\mathbf{s}_{t}+\\text { Unnormalize }\\left(f_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n","$$ \\\\\n","\n","You generate the reward with $$\n","\\hat{\\mathbf{r}}_{t}=\\text { Unnormalize }\\left(r_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n","$$\n","\n","\n","*Students have to fill in codes at places there are ##TODO comments.*\n","\n","**Note** We will use sklearn's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to perform normalization and unnormalization steps, through out this assignment."]},{"cell_type":"code","metadata":{"id":"hDOIWrcmO2LT","executionInfo":{"status":"ok","timestamp":1639125544029,"user_tz":-60,"elapsed":4,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def model_MSEloss(y_truth, y_pred, device):\n","    '''\n","    input_1: ground truth values (batch_size x data_dimension)\n","    input_2: predicted values (batch_size x data_dimension)\n","    input_3: device specification for pytorch ('cuda' / 'cpu)\n","\n","    return: mean squared error loss between\n","\n","    Compute the MSE (Mean Squared Error) between y_truth and y_pred \n","    (Tip: Use https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html)\n","    '''\n","    \n","    ### Your code starts here ###\n","    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n","    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n","    ### Your code ends here ###\n","    "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"4lZrey6pan0S","executionInfo":{"status":"ok","timestamp":1639125777658,"user_tz":-60,"elapsed":509,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def train_dyna_model(random_dataset, rl_dataset, env_model, rew_model, batch_size, max_model_iter, num_examples_added, ENV_LEARNING_RATE, REW_LEARNING_RATE, device):\n","    '''\n","    Train the two models that predict the next state and the expected reward\n","    '''\n","    print(\"................Training Dynamics and Reward Models With Collected Data So Far.....................\")\n","\n","    env_optimizer = optim.Adam(env_model.parameters(), lr=ENV_LEARNING_RATE)\n","    rew_optimizer = optim.Adam(rew_model.parameters(), lr=REW_LEARNING_RATE)\n","\n","    ###Accessing Inputs and Outputs to the Neural Network From Data Buffer D###\n","    if len(rl_dataset) > 0:\n","        '''\n","        # To use only a fraction of the random dataset\n","        rand = np.arange(len(random_dataset))\n","        np.random.shuffle(rand)\n","        rand = rand[:int(len(rl_dataset)*0.8)] # 80% of rl dataset\n","        d_concat = np.concatenate([np.array(random_dataset)[rand], rl_dataset], axis=0)'''\n","\n","        # Concatenate the random dataset with the RL dataset. Used only in the aggregation iterations\n","        d_concat = np.concatenate([random_dataset, rl_dataset], axis=0)\n","    else:\n","        d_concat = np.array(random_dataset)\n","\n","    # Split the dataset into train(80%) and test(20%)\n","    D_train = d_concat[:int(-num_examples_added*1/5)]\n","    D_valid = d_concat[int(-num_examples_added*1/5):]\n","\n","    print(\"len(D):\", len(d_concat), 'len(Dtrain)', len(D_train))\n","\n","    # Shuffle the dataset\n","    sff = np.arange(len(D_train))\n","    np.random.shuffle(sff)\n","    D_train = D_train[sff]\n","\n","    ### >>>>>>>>>Your code starts here<<<<<<<<< ###\n","    \n","    # Create the input and output for the train dynamics and reward models by accessing D_train and D_test\n","    X_train =    np.array([np.concatenate([obs,act]) for obs,_,_,act in D_train])    # Concatenate obs and action\n","    print(\"X_train:\\n\",X_train)\n","    X_train = torch.tensor(X_train)\n","    y_rew_train = np.array([[rw] for _,_,rw,_ in D_train])    # Reward model groundtruths\n","    y_next_state_train = np.array([no for _,no,_,_ in D_train])    # Next state output s(t+1)\n","    y_diff_train = y_next_state_train - np.array([obs for obs,_,_,_ in D_train])     # y(state) = s(t+1) - s(t) used as groundtruths for dynamics models \n","\n","    # Create the input and output array for the validation\n","    X_valid =    np.array([np.concatenate([obs,act]) for obs,_,_,act in D_valid])  # Concatenate obs and action\n","    y_rew_valid = np.array([[rw] for _,_,rw,_ in D_valid])     # Reward's output\n","    y_next_state_valid = np.array([no for _,no,_,_ in D_valid])  # Next state output\n","    y_diff_valid = y_next_state_valid - np.array([obs for obs,_,_,_ in D_valid])    # y(state) = s(t+1) - s(t)\n","\n","    ###>>>>>>>>>>>> Your code Ends here<<<<<<<<<< ###\n","\n","    ####Standardize The Inputs And Outputs ###\n","\n","    # Standardize the input features by removing the mean and scaling to unit variance\n","    input_scaler = StandardScaler()\n","    X_train = input_scaler.fit_transform(X_train)\n","    X_valid = input_scaler.transform(X_valid)\n","\n","    # Standardize the outputs by removing the mean and scaling to unit variance\n","\n","    env_output_scaler = StandardScaler()\n","    y_diff_train = env_output_scaler.fit_transform(y_diff_train)\n","    y_diff_valid = env_output_scaler.transform(y_diff_valid)\n","\n","    rew_output_scaler = StandardScaler()\n","    y_rew_train = rew_output_scaler.fit_transform(y_rew_train)\n","    y_rew_valid = rew_output_scaler.transform(y_rew_valid)\n","\n","    # store all the scalers in a variable to later uses\n","    norm = (input_scaler, env_output_scaler, rew_output_scaler)\n","\n","    losses_env = []\n","    losses_rew = []\n","\n","    # go through max_model_iter supervised iterations\n","    with tqdm(total=max_model_iter, position=0, leave=True) as pbar:\n","      for it in tqdm(range(max_model_iter),position=0, leave=True):\n","          # create mini batches of size batch_size\n","          for mb in range(0, len(X_train), batch_size):\n","\n","              if len(X_train) > mb+BATCH_SIZE:\n","                  X_mb = X_train[mb:mb+BATCH_SIZE]\n","\n","                  y_diff_mb = y_diff_train[mb:mb+BATCH_SIZE]\n","                  y_rew_mb = y_rew_train[mb:mb+BATCH_SIZE]\n","\n","                  # Add gaussian noise with mean 0 and variance 0.0001 as in the paper\n","                  X_mb += np.random.normal(loc=0, scale=0.001, size=X_mb.shape)\n","\n","                  ## Optimization of the 'env_model' neural net\n","\n","                  env_optimizer.zero_grad()\n","                  # forward pass of the model to compute the output\n","                  pred_state = env_model(torch.tensor(X_mb).to(device))\n","                  # compute the MSE loss\n","                  loss = model_MSEloss(y_diff_mb, pred_state, device)\n","\n","                  if it == (max_model_iter - 1):\n","                      losses_env.append(loss.cpu().detach().numpy())\n","\n","                  # backward pass\n","                  loss.backward()\n","                  # optimization step\n","                  env_optimizer.step()\n","\n","\n","                  ## Optimization of the 'rew_model' neural net\n","                  rew_optimizer.zero_grad()\n","                  # forward pass of the model to compute the output\n","                  pred_rew = rew_model(torch.tensor(X_mb).to(device))\n","                  # compute the MSE loss\n","                  loss = model_MSEloss(y_rew_mb, pred_rew, device)\n","\n","                  if it == (max_model_iter - 1):\n","                      losses_rew.append(loss.cpu().detach().numpy())\n","                  # backward pass\n","                  loss.backward()\n","                  # optimization step\n","                  rew_optimizer.step()\n","\n","      # Evalute the models every 10 iterations and print the losses\n","      if it % 10 == 0:\n","          env_model.eval()\n","          rew_model.eval()\n","\n","          pred_state = env_model(torch.tensor(X_valid).to(device))\n","          pred_rew = rew_model(torch.tensor(X_valid).to(device))\n","          env_model.train(True)\n","          rew_model.train(True)\n","\n","          valid_env_loss = model_MSEloss(y_diff_valid, pred_state, device)\n","          valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n","\n","          print('..', it, valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy())\n","\n","\n","    ## Evaluate the MSE losses\n","\n","    env_model.eval()\n","    rew_model.eval()\n","\n","    pred_state = env_model(torch.tensor(X_valid).to(device))\n","    pred_rew = rew_model(torch.tensor(X_valid).to(device))\n","    env_model.train(True)\n","    rew_model.train(True)\n","\n","    valid_env_loss = model_MSEloss(y_diff_valid, pred_state, device)\n","    valid_rew_loss = model_MSEloss(y_rew_valid, pred_rew, device)\n","\n","    return np.mean(losses_env), np.mean(losses_rew), valid_env_loss.cpu().detach().numpy(), valid_rew_loss.cpu().detach().numpy(), norm\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jh_C5RZ7bhQx"},"source":["#Planning With Dynamics And Reward Models\n","\n","Given the learned dynamics and reward models, we now want to select and execute actions that minimize a cost function (long term rewards). Ideally, you would calculate these actions by solving the following optimization:\n","\n","$$\n","\\mathbf{a}_{t}^{*}=\\arg \\min _{\\mathbf{a}_{t: \\infty}} \\sum_{t^{\\prime}=t}^{\\infty} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { where } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n","$$\n","\n","However, solving the above equation is impractical for two reasons: (1) planning over an infinite sequence of actions is impossible and (2) the learned dynamics model is imperfect, so using it to plan in such an open-loop manner will lead to accumulating errors over time and planning far into the future will become very inaccurate.\n","Instead, we will solve the following gradient-free optimization problem:\n","$$\n","\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n","$$\n","in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$. \n"]},{"cell_type":"markdown","metadata":{"id":"njLI42lYbuZc"},"source":["##The cost function for planners\n","\n","Here we calculate the cost function for the planners as\n","$$\n","\\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n","$$"]},{"cell_type":"code","metadata":{"id":"pPeynie8boLO","executionInfo":{"status":"ok","timestamp":1639125544348,"user_tz":-60,"elapsed":5,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def evaluate_cost(m_obs,sampled_action_sequences):\n","  # array that contains the rewards for all the sequence\n","  unroll_rewards = np.zeros((num_sequences, 1))\n","\n","  for t in range(horizon_length):\n","      # sampled actions for each sequence\n","      sampled_actions = sampled_action_sequences[:,t,:]\n","      # scale the input\n","      models_input = input_scaler.transform(np.concatenate([m_obs, sampled_actions], axis=1))\n","      # compute the next state for each sequence\n","      pred_obs = env_model(torch.tensor(models_input).to(device))\n","      # and the reward\n","      pred_rew = rew_model(torch.tensor(models_input).to(device))\n","\n","      # inverse scaler transofrmation\n","      pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n","      # and add previous observation\n","      m_obs = pred_obs + m_obs\n","\n","      assert(pred_rew.cpu().detach().numpy().shape == unroll_rewards.shape)\n","\n","      # sum of the expected rewards\n","      unroll_rewards += pred_rew.cpu().detach().numpy()\n","\n","  return unroll_rewards"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKOXz2MydCrL"},"source":["# Task 2: What is a Model Predictive Controller (MPC)?? (1 Pts)\n","\n","Since our dynamics and reward models can be imperfect and things will never go perfectly ac-cording to plan, we adopt a model predictive control (MPC) approach. Explain in few lines whats the basic priniciple behind an MPC.\n","\n","**Answer**:\n","In model predictive control (MPC) approach, we solve $$\n","\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n","$$ at every time step to select the best H-step action sequence, but then we execute only the first action from that sequence before replanning again at the next time step using updated state information."]},{"cell_type":"markdown","metadata":{"id":"aOFAOWMKb4Rj"},"source":["##Planner 1: Random Shooting Based Planner\n","\n","We will now use a simple random shooting method to solve the solve the following gradient-free optimization problem :\n","$$\n","\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n","$$\n","in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$.\n","\n","**Random Shooting**: The simplest gradient-free optimizer simply generates $N$ independent random action sequences $\\left\\{A_{0} \\ldots A_{N}\\right\\}$, where each sequence $A_{i}=\\left\\{a_{t}^{i} \\ldots a_{t+H-1}^{i}\\right\\}$ is of length $H$ action. Given a reward function $r(s, a)$ that defines the task, and given future state predictions $\\hat{s}_{t+1}=f_{\\theta}\\left(\\hat{s}_{t}, a_{t}\\right)+\\hat{s}_{t}$ from the learned dynamics model $f_{\\theta}$, the optimal action sequence $A_{i^{*}}$ is selected to be the one corresponding to the sequence with highest predicted reward: $i^{*}=$ $\\arg \\max _{i} R_{i}=\\arg \\max _{i} \\sum_{t^{\\prime}=t}^{t+H-1} r\\left(\\hat{s}_{t^{\\prime}}, a_{t^{\\prime}}^{i}\\right) .$ This approach has been shown to achieve success on continuous control tasks with learned models, but it has numerous drawbacks: it scales poorly with the dimension of both the planning horizon and the action space, and it often is insufficient for achieving high task performance since a sequence of actions sampled at random often does not directly lead to meaningful behavior. So in the next section we will implement a Cross Entropy Method (CEM) based planner, which proves to better for certain environments.\n","\n","Note that, since our model is imperfect and things will never go perfectly according to plan, we adopt a model predictive control (MPC) approach."]},{"cell_type":"code","metadata":{"id":"5yaL550Mb7sc","executionInfo":{"status":"ok","timestamp":1639125544349,"user_tz":-60,"elapsed":6,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def random_shooting_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, norm, device):\n","    '''\n","    Use a random-sampling shooting method, generating random action sequences. The first action with the highest reward of the entire sequence is returned\n","    '''\n","    best_reward = -1e9\n","    best_next_action = []\n","\n","    input_scaler, env_output_scaler, rew_output_scaler = norm\n","\n","    m_obs = np.array([real_obs for _ in range(num_sequences)])\n","\n","    # array that contains the cumilative rewards for all the sequence\n","    cumilative_rewards = np.zeros((num_sequences, 1))\n","    first_sampled_actions = []\n","\n","    env_model.eval()\n","    rew_model.eval()\n","\n","    ## Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon_length' times.\n","    ## i.e. roll a given number of trajectories in a single batch (to increase speed)\n","\n","    # sampled actions for each sequence\n","    sampled_action_sequences = []\n","    for _ in range(num_sequences):\n","      sampled_action_sequence = [sample_action() for _ in range(horizon_length)]\n","      sampled_action_sequences.append(sampled_action_sequence)\n","\n","    sampled_action_sequences = np.array(sampled_action_sequences)\n","\n","\n","    for t in range(horizon_length):\n","      # select action for time step t for each sequence\n","      sampled_actions = sampled_action_sequences[:,t,:]\n","      # scale the input\n","      models_input = input_scaler.transform(np.concatenate([m_obs, sampled_actions], axis=1))\n","      # compute the next state for each sequence\n","      pred_obs = env_model(torch.tensor(models_input).to(device))\n","      # and the reward\n","      pred_rew = rew_model(torch.tensor(models_input).to(device))\n","\n","      # inverse scaler transofrmation\n","      pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n","      # and add previous observation\n","      m_obs = pred_obs + m_obs\n","\n","      assert(pred_rew.cpu().detach().numpy().shape == cumilative_rewards.shape)\n","\n","      # sum of the expected rewards\n","      cumilative_rewards += pred_rew.cpu().detach().numpy()\n","\n","      if t == 0:\n","        first_sampled_actions = sampled_actions\n","\n","    env_model.train(True)\n","    rew_model.train(True)\n","\n","    # Best the position of the sequence with the higher reward\n","    arg_best_reward = np.argmax(cumilative_rewards)\n","    best_sum_reward = cumilative_rewards[arg_best_reward].squeeze()\n","    # take the first action of this sequence\n","    best_action = first_sampled_actions[arg_best_reward]\n","\n","    return best_action, best_sum_reward"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8r_3mKZ1b8Rd"},"source":["# Task 3 (5 Pts)\n","##Implement Cross Entropy Method (CEM) Based Planner\n","\n","We will now use a simple CEM method to solve the solve the following gradient-free optimization problem :\n","$$\n","\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n","$$\n","in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$\n","\n","**Cross-Entropy Method** (`CEM`) is an optimization algorithm applicable to problems that are both **combinatorial** and **continuous**, which is our case: find the best performing sequence of actions.\n","\n","Cross-entropy method (CEM) approach, begins as the random shooting approach, but then does this sampling for multiple iterations $m \\in\\{0 \\ldots M\\}$ at each time step. The top $J$ highest-scoring action sequences from each iteration are used to update and refine the mean and variance of the sampling distribution for the next iteration, as follows:\n","\n","\\begin{aligned}\n","A_{i} &=\\left\\{a_{0}^{i} \\ldots a_{H-1}^{i}\\right\\} \\text {, where } a_{t}^{i} \\sim \\mathcal{N}\\left(\\mu_{t}^{m}, \\Sigma_{t}^{m}\\right) \\forall i \\in N, t \\in 0 \\ldots H-1 \\\\\n","A_{\\text {elites }} &=\\operatorname{sort}\\left(A_{i}\\right)[-J:] \\\\\n","\\mu_{t}^{m+1} &=  \\operatorname{mean}\\left(A_{\\text {elites }}\\right) \\quad \\forall t \\in 0 \\ldots H-1 \\\\\n","\\Sigma_{t}^{m+1} &= \\operatorname{var}\\left(A_{\\text {elites }}\\right)\\quad  \\forall t \\in 0 \\ldots H-1\n","\\end{aligned}\n","\n","After $M$ iterations, the optimal actions are selected to be the resulting mean of the action distribution.\n","\n","Note that we use an MPC planner which replans at every time step similar to previous section with random shooting planner. Students may refer to [this paper](/https://arxiv.org/pdf/1909.11652.pdf).\n","\n","*Your task is to fill in your code at all the lines with ##TODO comment in it. Students may refer to the random shooting method in the previous section as many aspects of the code would be similar to this*"]},{"cell_type":"code","metadata":{"id":"W3x-Bvy1cA5_","executionInfo":{"status":"ok","timestamp":1639125544661,"user_tz":-60,"elapsed":317,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["def cem_based_control(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, norm, device):\n","    '''\n","    Use a cem method for planning\n","    '''\n","    best_reward = -1e9\n","    best_next_action = []\n","    k = 2 #Number Of Elites To Select\n","    optimisation_iters = 3\n","\n","    input_scaler, env_output_scaler, rew_output_scaler = norm\n","\n","    first_sampled_actions = []\n","\n","    env_model.eval()\n","    rew_model.eval()\n","\n","    ## Create a batch of size 'num_sequences' (number of trajectories) to roll the models 'horizon_length' times.\n","    ## i.e. roll a given number of trajectories in a single batch (to increase speed)\n","\n","    # sampled actions for each sequence each sequence is of length H.\n","    sampled_action_sequences = []\n","    for _ in range(num_sequences):\n","        sampled_action_sequence = [sample_action() for _ in range(horizon_length)]\n","        sampled_action_sequences.append(sampled_action_sequence)\n","\n","    sampled_action_sequences = np.array(sampled_action_sequences)\n","    action_dim = sampled_action_sequences.shape[-1]\n","    ### >>>>>>>>>Your code starts here<<<<<<<<< ###\n","\n","    action_mean, action_std_dev = sampled_action_sequences.mean(axis=0), sampled_action_sequences.std(axis=0)    # Calculate the mean and standard deviation of the sampled action sequences\n","\n","    for _ in range(optimisation_iters):\n","        '''\n","        In this section you implement an MPC with CEM method.  In each optimization iteration,\n","        you sample actions sequences from a normal distribution, Calculate the cost for each\n","        sequence using the learnt dyanmics and reward models. We will use a MPC similar to previous\n","        section to replan at every timestep.\n","        '''\n","        sampled_action_sequences = (\n","                    action_mean + action_std_dev * np.random.randn(num_sequences, horizon_length, action_dim))  # Sample from a Gaussian Using The Means and Standard Deviations\n","        # array that contains the cumilative_rewards for all the sequence, set to zero intially before an optimization iteration\n","        cumilative_rewards = np.zeros((num_sequences, 1))\n","        m_obs = np.array([real_obs for _ in range(num_sequences)])\n","\n","        for t in range(horizon_length):\n","            # sampled actions for each sequence\n","            sampled_actions = sampled_action_sequences[:, t, :]\n","            # scale the input\n","            models_input = input_scaler.transform(np.concatenate([m_obs, sampled_actions], axis=1))\n","\n","            \n","            pred_obs = env_model(torch.tensor(models_input).to(device)) # compute the differences to the next state using the dynamics model\n","            \n","            pred_rew =  rew_model(torch.tensor(models_input).to(device)) # compute the reward using the reward model\n","            \n","            pred_obs_unnormlized = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())  # inverse scaler transofrmation (Unnormalize the predicted differences)\n","            \n","            m_obs = pred_obs_unnormlized + m_obs # and add pred_unnormalized_observation to the previous observation\n","\n","            assert (pred_rew.cpu().detach().numpy().shape == cumilative_rewards.shape)\n","\n","            # sum of the expected rewards\n","            cumilative_rewards += pred_rew.cpu().detach().numpy()   #TODO #keep on adding the rewards obtained so far\n","\n","            if t == 0:\n","                first_sampled_actions = sampled_actions\n","            \n","        #Select Top K Action Sequences (lets call them elite_sequences) that gave the highest cumilative reward\n","        topk = np.argsort(cumilative_rewards[:, 0])[::-1][:10]\n","        elite_sequences = sampled_action_sequences[topk, :, :]\n","\n","\n","        #Recalculate the mean and variances using the elite (topk) action sequences \n","        action_mean, action_std_dev = elite_sequences.mean(axis=0), elite_sequences.std(axis=0)\n","\n","    ### >>>>>>>>>Your code ends here<<<<<<<<< ###\n","    env_model.train(True)\n","    rew_model.train(True)\n","\n","    # Best the position of the sequence with the higher reward\n","    arg_best_reward = np.argmax(cumilative_rewards)\n","    best_sum_reward = cumilative_rewards[arg_best_reward].squeeze()\n","    # take the first action of this sequence\n","    best_action = first_sampled_actions[arg_best_reward] #you can also choose the best action as the mean of the elites from last optimization iteration. \n","                                                          # However we do a greedy step in the end where the best among the elites from last iteration is chosen.\n","\n","    return best_action, best_sum_reward\n"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RzYWX5HcYVs"},"source":["#Main Loop (1 Pts)\n","\n","Under Default Hyperparameters, Run the Main MBRL Loop with two planners Random Shooting and CEM. You will compare the reward plots with both the planners.\n","\n","\\begin{aligned}\n","&\\overline{\\text { Algorithm } \\mathbf{1} \\text { Model-based Reinforcement Learning }} \\\\\n","&\\hline \\text { 1: gather dataset } \\mathcal{D}_{\\text {RAND }} \\text { of random trajectories } \\\\\n","&\\text { 2: initialize empty dataset } \\mathcal{D}_{\\text {RL }} \\text {, and randomly initialize } f_{\\theta} \\\\\n","&  \\text{ 3: } \\textbf{for} \\text{ iter=1 to max_iter } \\textbf{do}  \\\\\n","&\\quad  \\quad \\quad \\textbf {Model Learning } \\\\\n","&\\text { 4:} \\quad  \\quad \\text{ train } f_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text{ and } r_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text { by performing gradient descent on MSE Loss }  \\\\\n","& \\quad \\quad  \\quad \\text { using } \\mathcal{D}_{\\text {RAND }} \\text { and } \\mathcal{D}_{\\text {RL }} \\\\\n","&\\quad  \\quad \\quad \\textbf {Planning and More Experience Collection } \\\\\n","&\\text { 5: } \\quad  \\quad  \\textbf { for } t=1 \\text { to } T \\textbf { do } \\\\\n","&\\text { 6: } \\quad \\quad \\quad \\quad \\text { get agent's current state } \\mathbf{s}_{t} \\\\\n","&\\text { 7: } \\quad \\quad \\quad \\quad \\text { use } f_{\\theta} \\text { and } r_{\\theta} \\text { to estimate optimal action sequence } \\mathbf{A}_{t}^{(H)} \\\\\n","&\\text { 8: } \\quad \\quad \\quad \\quad \\text { execute first action } \\mathbf{a}_{t} \\text { from selected action sequence } \\\\\n","&\\text { 9: } \\quad \\quad \\quad \\quad\\text { Add } \\{s_t,  s_{t+1}, r_t, a_t\\} \\text { to } \\mathcal{D}_{\\text {RL }} \\\\\n","&\\text { 10: } \\quad \\text { end for } \\\\\n","&\\text { 11: end for } \\\\\n","&\\hline\n","\\end{aligned}\n","\n","**Students should get two reward plots, one with the random shooting based planner (set planner = 'random_shooting') and another one with CEM planner (set planner = 'cem'). Expect greater than 500 reward for both cases in 20 iterations. Please submit the saved figures for both planners with the default parameters given here.**"]},{"cell_type":"code","metadata":{"id":"9S1ymuhOySCN","executionInfo":{"status":"ok","timestamp":1639125544661,"user_tz":-60,"elapsed":2,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"source":["# 'cuda' or 'cpu'\n","device = 'cuda'\n","planner = 'cem' #Set this to 'cem' if you want use CEM Planner"],"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Main loop hyper\n","# Do Not Change This Block\n","# Do not change This Block\n","\n","AGGR_ITER = 20 #Number Of Outer iterations \n","STEPS_PER_AGGR = 500 #Minimum Number of steps of experiences to add to buffer in each iteration\n","\n","# Number of Randon Trajectories (Experiences) To Collect for $D_{random}$\n","NUM_RAND_TRAJECTORIES = 100\n","\n","\n","# Supervised Model Hyperp\n","ENV_LEARNING_RATE = 1e-3\n","REW_LEARNING_RATE = 1e-3\n","BATCH_SIZE = 2000\n","TRAIN_ITER_MODEL = 55\n","\n","# Controller Hyperp\n","HORIZION_LENGTH = 15\n","NUM_ACTIONS_SEQUENCES = 200\n","\n","save_video_test = True\n","\n","now = datetime.datetime.now()\n","date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n","env = gym.make('InvertedPendulumSwingupPyBulletEnv-v0')\n","obs = env.reset()\n","\n","#Step 1:  gather the dataset of random sequences\n","print(\">>>>>>>>>>>>>>Gathering Random Trajectories to train Dynamics and Reward Models\")\n","rand_dataset = gather_random_trajectories(NUM_RAND_TRAJECTORIES, env)\n","\n","rl_dataset = []\n","mean_rew_list = []\n","\n","#Step 2: Initialize the models and itarate over model learning , planning and on policy experience collection\n","env_model = NNDynamicModel(env.action_space.shape[0] + env.observation_space.shape[0],\n","                          env.observation_space.shape[0]).to(device)\n","rew_model = NNRewardModel(env.action_space.shape[0] + env.observation_space.shape[0], 1).to(device)\n","\n","game_reward = 0\n","num_examples_added = len(rand_dataset)\n","\n","for n_iter in range(AGGR_ITER):\n","\n","  # supervised training of the dataset (random and rl if it exists)\n","  train_env_loss, train_rew_loss, valid_env_loss, valid_rew_loss, norm = train_dyna_model(rand_dataset,\n","                                                                                          rl_dataset,\n","                                                                                          env_model,\n","                                                                                          rew_model,\n","                                                                                          BATCH_SIZE,\n","                                                                                          TRAIN_ITER_MODEL,\n","                                                                                          num_examples_added,\n","                                                                                          ENV_LEARNING_RATE,\n","                                                                                          REW_LEARNING_RATE,\n","                                                                                          device)\n","  print('{} >> Eloss:{:.4f} EV loss:{:.4f} -- Rloss:{:.4f} RV loss:{:.4f}'.format(n_iter, train_env_loss,\n","                                                                                  valid_env_loss,\n","                                                                                  train_rew_loss,\n","                                                                                  valid_rew_loss))\n","  env = wrap_env(gym.make('InvertedPendulumSwingupPyBulletEnv-v0'))\n","  obs = env.reset()\n","\n","  num_examples_added = 0\n","  game_reward = 0\n","  game_pred_rews = []\n","  rews = []\n","\n","  while num_examples_added < STEPS_PER_AGGR:\n","      while True:\n","          tt = time.time()\n","          # Execute the control to roll the sequences and pick the first action of the sequence with the higher reward\n","          if planner is 'random_shooting':\n","              action, pred_rew = random_shooting_based_control(env_model, rew_model, obs, NUM_ACTIONS_SEQUENCES,\n","                                                    HORIZION_LENGTH, env.action_space.sample, norm, device)\n","          elif planner is 'cem':\n","              action, pred_rew = cem_based_control(env_model, rew_model, obs, NUM_ACTIONS_SEQUENCES,\n","                                                    HORIZION_LENGTH, env.action_space.sample, norm, device)\n","          else:\n","              raise ValueError(\"planner must be random_shooting/cem\")\n","          game_pred_rews.append(pred_rew)\n","\n","          # one step in the environment with the action returned by the controller\n","          new_obs, reward, done, _ = env.step(action)\n","\n","          input_scaler, env_output_scaler, rew_output_scaler = norm\n","\n","          ## Compute the reward and print some stats\n","          models_input = input_scaler.transform([np.concatenate([obs, action])])\n","          rew_model.eval()\n","          p_rew = rew_model(torch.tensor(models_input).to(device))\n","          rew_model.train(True)\n","          unnorm_rew = rew_output_scaler.inverse_transform(p_rew.cpu().detach().numpy())\n","          if num_examples_added == 0:\n","              print('Steps taken with MPC Planner:', num_examples_added + 1, end='->')\n","          elif num_examples_added % 25 == 0:\n","              print(num_examples_added + 1, end='->')\n","          rl_dataset.append([obs, new_obs, reward, action])\n","\n","          num_examples_added += 1\n","          obs = new_obs\n","          game_reward += reward\n","\n","          # if the environment is done, reset it and print some stats\n","          if done:\n","              print(num_examples_added + 1, end='->')\n","              obs = env.reset()\n","              \n","              rews.append(game_reward)\n","              game_reward = 0\n","              game_pred_rews = []\n","              break\n","\n","  print('#########################Total Episodic Reward Obtained With Current Policy At Iteration',n_iter,' = ', rews)\n","  mean_rew_list.append(rews)\n","  env.close()\n","  show_video()\n","\n","## Plot the Reward Curves\n","fig, ax = plt.subplots(1, figsize=(10,6))\n","sns.set_style(\"darkgrid\")\n","idxs = range(len(mean_rew_list))\n","plt.plot(idxs, mean_rew_list)\n","if planner is \"random_shooting\":\n","    plt.title('Random Shooting')\n","else:\n","    plt.title('CEM Control')\n","plt.ylabel('Rewards')\n","plt.xlabel('Iterations')\n","save_figure('Deep_Dynamics_Reward_'+planner)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":869},"id":"FRIS_WMayXkc","outputId":"36b49e7c-7eea-49a9-aeb3-1115b1231665","executionInfo":{"status":"error","timestamp":1639125921636,"user_tz":-60,"elapsed":46093,"user":{"displayName":"Samuel Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0VIDnQ4r_zr1rtCoFPzvAIa4u4Ph7j4kBIzlq0g=s64","userId":"14101473320852980185"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]},{"output_type":"stream","name":"stdout","text":[">>>>>>>>>>>>>>Gathering Random Trajectories to train Dynamics and Reward Models\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:24<00:00,  4.04it/s]\n","100%|██████████| 100/100 [00:24<00:00,  4.03it/s]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"]},{"output_type":"stream","name":"stdout","text":["Mean R: -933.29 Max R: -0.21 1000.0\n","................Training Dynamics and Reward Models With Collected Data So Far.....................\n","len(D): 100000 len(Dtrain) 80000\n","X_train:\n"," [[-0.95060095  0.11560829 -0.9644557  -0.26424459 -1.08964516 -0.77246463]\n"," [ 1.00090005 -0.01363714 -0.99315556  0.11679912 -0.20816665  0.96668935]\n"," [ 0.79824701  0.78000438 -0.9318841  -0.36275616 -0.28245262 -0.40707266]\n"," ...\n"," [ 0.91958139 -0.16522936 -0.99970081 -0.02446016  0.10842223 -0.34067911]\n"," [ 0.3477622  -0.31869579 -0.99151832 -0.12996702 -0.24553004  0.2079667 ]\n"," [-0.52629244 -0.15186777 -0.9970987  -0.07611956 -0.34534322 -0.1738002 ]]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 55/55 [00:17<00:00,  3.08it/s]\n","  0%|          | 0/55 [00:17<?, ?it/s]\n","/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]},{"output_type":"stream","name":"stdout","text":["0 >> Eloss:0.1263 EV loss:0.1255 -- Rloss:0.0026 RV loss:0.0013\n","Steps taken with MPC Planner: 1->"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-fa4cd28a8d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0;32melif\u001b[0m \u001b[0mplanner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m'cem'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m               action, pred_rew = cem_based_control(env_model, rew_model, obs, NUM_ACTIONS_SEQUENCES,\n\u001b[0;32m---> 78\u001b[0;31m                                                     HORIZION_LENGTH, env.action_space.sample, norm, device)\n\u001b[0m\u001b[1;32m     79\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m               \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"planner must be random_shooting/cem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-208441029c8a>\u001b[0m in \u001b[0;36mcem_based_control\u001b[0;34m(env_model, rew_model, real_obs, num_sequences, horizon_length, sample_action, norm, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msampled_action_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msampled_action_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0msampled_action_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_action_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-208441029c8a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msampled_action_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msampled_action_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0msampled_action_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_action_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         sample[bounded] = self.np_random.uniform(low=self.low[bounded],\n\u001b[1;32m    118\u001b[0m                                             \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbounded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                                             size=bounded[bounded].shape)\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'i'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.uniform\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mall\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_all_dispatcher\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2333\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_all_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"4jNngsO5A4iR"},"source":["### **Note:** Though we focussed on planners for this homework and used a rather simple dynamics model based on feedforward neural networks, the real difficult problem is learning a good dynamics model. There are several interesting research in this direction. Some interesting references are provided below for your information:\n","\n","[Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models](https://arxiv.org/abs/1912.01603)\n","\n","[Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603)\n","\n","[Contrastive Learning of Structured World Models](https://arxiv.org/abs/1911.12247)\n","\n","[World Models](https://arxiv.org/abs/1803.10122)"]}]}